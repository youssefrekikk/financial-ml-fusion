{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hidden Markov Model for Market Regime Detection\n",
    "## Quantitative Analysis & Model Validation\n",
    "### Not complete yet\n",
    "\n",
    "This notebook provides an  analysis of the Hidden Markov Model (HMM) implementation for market regime detection. We'll explore the model's design decisions, parameter optimization, and performance characteristics through statistical validation techniques.\n",
    "\n",
    "**Key areas of investigation:**\n",
    "1. Data preprocessing and feature engineering\n",
    "2. Model parameter selection and optimization\n",
    "3. Regime identification and characterization\n",
    "4. Statistical significance testing\n",
    "5. Performance across different market conditions\n",
    "6. Robustness to parameter changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import date, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('fivethirtyeight')\n",
    "sns.set_palette(\"deep\")\n",
    "\n",
    "# Import custom modules\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from src.improved_hmm_model import ImprovedHMMModel\n",
    "from src.utils.data_loader import load_data\n",
    "from src.utils.feature_engineering import add_features, normalize_features\n",
    "from src.utils.permutation_tests import plot_walk_forward_permutation_results\n",
    "from src.utils.monte_carlo import plot_monte_carlo_results\n",
    "\n",
    "# Configure notebook display\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Acquisition and Preprocessing\n",
    "\n",
    "We'll begin by loading historical price data for multiple assets to ensure our analysis is robust across different market types. We'll examine both equity indices and individual stocks to validate the model's versatility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tickers to analyze\n",
    "tickers = [\n",
    "    'SPY',    # S&P 500 ETF\n",
    "    'QQQ',    # Nasdaq 100 ETF\n",
    "    'AAPL',   # Large cap tech\n",
    "    'XLE',    # Energy sector ETF\n",
    "    'GLD'     # Gold ETF (different asset class)\n",
    "]\n",
    "\n",
    "# Define date ranges to capture different market regimes\n",
    "start_date = \"2010-01-01\"  # Long enough to include multiple market cycles\n",
    "end_date = date.today().strftime('%Y-%m-%d')\n",
    "\n",
    "# Load data for each ticker\n",
    "data_dict = {}\n",
    "for ticker in tickers:\n",
    "    print(f\"Loading data for {ticker}...\")\n",
    "    data = load_data(ticker, start_date, end_date, use_local=True)\n",
    "    data_dict[ticker] = data\n",
    "    \n",
    "# Display summary statistics for each dataset\n",
    "for ticker, data in data_dict.items():\n",
    "    print(f\"\\n{ticker} Data Summary:\")\n",
    "    print(f\"Date Range: {data.index.min().date()} to {data.index.max().date()}\")\n",
    "    print(f\"Trading Days: {len(data)}\")\n",
    "    print(f\"Missing Values: {data.isna().sum().sum()}\")\n",
    "    \n",
    "# Select SPY as our primary analysis dataset\n",
    "spy_data = data_dict['SPY']\n",
    "spy_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Data Quality Analysis\n",
    "\n",
    "Before proceeding with modeling, we need to ensure our data is clean and suitable for analysis. Let's examine the data for anomalies, missing values, and other potential issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to analyze data quality\n",
    "def analyze_data_quality(data, ticker):\n",
    "    # Check for missing values\n",
    "    missing = data.isna().sum()\n",
    "    \n",
    "    # Check for outliers in returns\n",
    "    returns = data['Close'].pct_change()\n",
    "    outliers = returns[abs(returns) > returns.std() * 3]\n",
    "    \n",
    "    # Check for gaps in trading days\n",
    "    date_diffs = data.index.to_series().diff().dt.days\n",
    "    gaps = date_diffs[date_diffs > 3]  # More than 3 days between observations\n",
    "    \n",
    "    # Visualize price history\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.plot(data.index, data['Close'])\n",
    "    plt.title(f'{ticker} Price History')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Price')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    # Visualize return distribution\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    sns.histplot(returns.dropna(), kde=True)\n",
    "    plt.axvline(0, color='r', linestyle='--')\n",
    "    plt.title(f'{ticker} Daily Return Distribution')\n",
    "    plt.xlabel('Daily Return')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    # Return summary\n",
    "    return {\n",
    "        'missing_values': missing,\n",
    "        'outliers': len(outliers),\n",
    "        'outlier_dates': outliers.index.tolist(),\n",
    "        'gaps': len(gaps),\n",
    "        'gap_dates': gaps.index.tolist()\n",
    "    }\n",
    "\n",
    "# Analyze SPY data quality\n",
    "spy_quality = analyze_data_quality(spy_data, 'SPY')\n",
    "print(\"Data Quality Analysis:\")\n",
    "print(f\"Missing Values: {spy_quality['missing_values'].sum()}\")\n",
    "print(f\"Outliers: {spy_quality['outliers']}\")\n",
    "print(f\"Gaps in Trading Days: {spy_quality['gaps']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Feature Engineering Analysis\n",
    "\n",
    "The HMM model relies on carefully engineered features to detect market regimes. Let's examine the features created by our feature engineering pipeline and assess their predictive power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add features to SPY data\n",
    "spy_features = add_features(spy_data)\n",
    "\n",
    "# Display available features\n",
    "print(\"Available Features:\")\n",
    "feature_columns = [col for col in spy_features.columns if col not in ['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume']]\n",
    "print(feature_columns)\n",
    "\n",
    "# Analyze feature correlations\n",
    "correlation_matrix = spy_features[feature_columns].corr()\n",
    "\n",
    "# Plot correlation heatmap\n",
    "plt.figure(figsize=(16, 14))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n",
    "plt.title('Feature Correlation Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analyze feature distributions\n",
    "plt.figure(figsize=(16, 12))\n",
    "for i, feature in enumerate(feature_columns[:12]):  # Plot first 12 features\n",
    "    plt.subplot(4, 3, i+1)\n",
    "    sns.histplot(spy_features[feature].dropna(), kde=True)\n",
    "    plt.title(feature)\n",
    "    plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Feature Selection Analysis\n",
    "\n",
    "The model uses SelectKBest with f_regression to select the most informative features. Let's analyze which features are consistently selected and their predictive power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize HMM model with feature selection enabled\n",
    "hmm_model = ImprovedHMMModel(feature_selection=True, k_features=8)\n",
    "\n",
    "# Create target variable (5-day forward returns)\n",
    "spy_features['Forward_Return_5d'] = spy_features['Close'].pct_change(5).shift(-5)\n",
    "\n",
    "# Prepare features for selection\n",
    "X = spy_features[feature_columns].dropna()\n",
    "y = spy_features['Forward_Return_5d'].loc[X.index].dropna()\n",
    "\n",
    "# Align X and y\n",
    "common_idx = X.index.intersection(y.index)\n",
    "X = X.loc[common_idx]\n",
    "y = y.loc[common_idx]\n",
    "\n",
    "# Perform feature selection\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "selector = SelectKBest(f_regression, k=8)\n",
    "X_selected = selector.fit_transform(X, y)\n",
    "\n",
    "# Get feature scores\n",
    "feature_scores = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'F_Score': selector.scores_,\n",
    "    'P_Value': selector.pvalues_\n",
    "})\n",
    "feature_scores = feature_scores.sort_values('F_Score', ascending=False)\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='F_Score', y='Feature', data=feature_scores)\n",
    "plt.title('Feature Importance (F-Score)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display selected features\n",
    "selected_features = feature_scores.head(8)['Feature'].tolist()\n",
    "print(\"Selected Features:\")\n",
    "for i, feature in enumerate(selected_features, 1):\n",
    "    print(f\"{i}. {feature}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. HMM Model Parameter Optimization\n",
    "\n",
    "The HMM model has several key parameters that need to be optimized, including the number of states and covariance type. Let's analyze the parameter selection process and validate the chosen parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate HMM parameters\n",
    "def evaluate_hmm_parameters(data, n_components_range, covariance_types):\n",
    "    results = []\n",
    "    \n",
    "    # Add features\n",
    "    data_with_features = add_features(data)\n",
    "    \n",
    "    # Prepare HMM features\n",
    "    hmm_model = ImprovedHMMModel(feature_selection=False)  # Disable feature selection for this test\n",
    "    hmm_features = hmm_model.prepare_hmm_features(data_with_features)\n",
    "    \n",
    "    # Normalize features\n",
    "    normalized_features, _ = normalize_features(hmm_features.values, fit=True)\n",
    "    \n",
    "    # Test different parameters\n",
    "    from hmmlearn.hmm import GaussianHMM\n",
    "    from sklearn.metrics import silhouette_score\n",
    "    \n",
    "    for n_components in n_components_range:\n",
    "        for covariance_type in covariance_types:\n",
    "            \n",
    "            try:\n",
    "                # Train model\n",
    "                model = GaussianHMM(\n",
    "                    n_components=n_components,\n",
    "                    covariance_type=covariance_type,\n",
    "                    n_iter=1000,\n",
    "                    random_state=42)\n",
    "                model.fit(normalized_features)\n",
    "                \n",
    "                # Predict states\n",
    "                states = model.predict(normalized_features)\n",
    "                \n",
    "                # Calculate silhouette score\n",
    "                silhouette = silhouette_score(normalized_features, states)\n",
    "                \n",
    "                # Calculate BIC\n",
    "                bic = model.bic(normalized_features)\n",
    "                \n",
    "                # Calculate log likelihood\n",
    "                log_likelihood = model.score(normalized_features)\n",
    "                \n",
    "                # Store results\n",
    "                results.append({\n",
    "                    'n_components': n_components,\n",
    "                    'covariance_type': covariance_type,\n",
    "                    'silhouette': silhouette,\n",
    "                    'bic': bic,\n",
    "                    'log_likelihood': log_likelihood\n",
    "                })\n",
    "                \n",
    "                print(f\"n_components={n_components}, covariance_type={covariance_type}, silhouette={silhouette:.4f}, BIC={bic:.2f}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error with n_components={n_components}, covariance_type={covariance_type}: {e}\")\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Evaluate HMM parameters on SPY data\n",
    "param_results = evaluate_hmm_parameters(\n",
    "    spy_data, \n",
    "    n_components_range=range(2, 6),\n",
    "    covariance_types=[\"full\", \"diag\", \"tied\", \"spherical\"]\n",
    ")\n",
    "\n",
    "# Plot parameter evaluation results\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Plot silhouette scores\n",
    "for cov_type in param_results['covariance_type'].unique():\n",
    "    subset = param_results[param_results['covariance_type'] == cov_type]\n",
    "    axes[0].plot(subset['n_components'], subset['silhouette'], marker='o', label=cov_type)\n",
    "axes[0].set_title('Silhouette Score by Number of States')\n",
    "axes[0].set_xlabel('Number of States')\n",
    "axes[0].set_ylabel('Silhouette Score')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot BIC\n",
    "for cov_type in param_results['covariance_type'].unique():\n",
    "    subset = param_results[param_results['covariance_type'] == cov_type]\n",
    "    axes[1].plot(subset['n_components'], subset['bic'], marker='o', label=cov_type)\n",
    "axes[1].set_title('BIC by Number of States (Lower is Better)')\n",
    "axes[1].set_xlabel('Number of States')\n",
    "axes[1].set_ylabel('BIC')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot log likelihood\n",
    "for cov_type in param_results['covariance_type'].unique():\n",
    "    subset = param_results[param_results['covariance_type'] == cov_type]\n",
    "    axes[2].plot(subset['n_components'], subset['log_likelihood'], marker='o', label=cov_type)\n",
    "axes[2].set_title('Log Likelihood by Number of States (Higher is Better)')\n",
    "axes[2].set_xlabel('Number of States')\n",
    "axes[2].set_ylabel('Log Likelihood')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find optimal parameters\n",
    "best_silhouette = param_results.loc[param_results['silhouette'].idxmax()]\n",
    "best_bic = param_results.loc[param_results['bic'].idxmin()]\n",
    "best_likelihood = param_results.loc[param_results['log_likelihood'].idxmax()]\n",
    "\n",
    "print(\"\\nOptimal Parameters:\")\n",
    "print(f\"Best by Silhouette: {best_silhouette['n_components']} states, {best_silhouette['covariance_type']} covariance\")\n",
    "print(f\"Best by BIC: {best_bic['n_components']} states, {best_bic['covariance_type']} covariance\")\n",
    "print(f\"Best by Log Likelihood: {best_likelihood['n_components']} states, {best_likelihood['covariance_type']} covariance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Cross-Validation of HMM Parameters\n",
    "\n",
    "To ensure our parameter selection is robust, let's perform time series cross-validation to validate the stability of our parameter choices across different time periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform time series cross-validation for HMM parameters\n",
    "def ts_cross_validate_hmm(data, n_splits=5):\n",
    "    from sklearn.model_selection import TimeSeriesSplit\n",
    "    \n",
    "    # Create time series splits\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    \n",
    "    # Add features\n",
    "    data_with_features = add_features(data)\n",
    "    \n",
    "    # Prepare HMM features\n",
    "    hmm_model = ImprovedHMMModel(feature_selection=True, k_features=8)\n",
    "    hmm_features = hmm_model.prepare_hmm_features(data_with_features)\n",
    "    \n",
    "    # Create target for feature selection\n",
    "    y = data_with_features['Close'].pct_change(5).shift(-5)\n",
    "    \n",
    "    # Results storage\n",
    "    cv_results = []\n",
    "    \n",
    "    # Perform cross-validation\n",
    "    for i, (train_idx, test_idx) in enumerate(tscv.split(hmm_features)):\n",
    "        print(f\"\\nFold {i+1}/{n_splits}\")\n",
    "        \n",
    "        # Split data\n",
    "        train_features = hmm_features.iloc[train_idx]\n",
    "        train_y = y.iloc[train_idx]\n",
    "        test_features = hmm_features.iloc[test_idx]\n",
    "        test_y = y.iloc[test_idx]\n",
    "        \n",
    "        # Train HMM model\n",
    "        fold_model = ImprovedHMMModel(n_components_range=(2, 5), n_splits=3)\n",
    "        \n",
    "        try:\n",
    "            # Fit model on training data\n",
    "            train_data_fold = data.iloc[train_idx]\n",
    "            fold_model.fit(train_data_fold)\n",
    "            \n",
    "            # Get optimal parameters for this fold\n",
    "            optimal_params = fold_model.best_params\n",
    "            \n",
    "            # Store results\n",
    "            cv_results.append({\n",
    "                'fold': i+1,\n",
    "                'train_size': len(train_idx),\n",
    "                'test_size': len(test_idx),\n",
    "                'n_components': optimal_params['n_components'],\n",
    "                'covariance_type': optimal_params['covariance_type'],\n",
    "                'bull_state': fold_model.bull_state,\n",
    "                'bear_state': fold_model.bear_state,\n",
    "                'neutral_state': fold_model.neutral_state\n",
    "            })\n",
    "            \n",
    "            print(f\"Optimal parameters: {optimal_params['n_components']} states, {optimal_params['covariance_type']} covariance\")\n",
    "            print(f\"Bull state: {fold_model.bull_state}, Bear state: {fold_model.bear_state}, Neutral state: {fold_model.neutral_state}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in fold {i+1}: {e}\")\n",
    "    \n",
    "    return pd.DataFrame(cv_results)\n",
    "\n",
    "# Run time series cross-validation\n",
    "cv_results = ts_cross_validate_hmm(spy_data, n_splits=5)\n",
    "\n",
    "# Analyze cross-validation results\n",
    "print(\"\\nCross-Validation Results:\")\n",
    "print(cv_results)\n",
    "\n",
    "# Count parameter frequency\n",
    "n_components_counts = cv_results['n_components'].value_counts()\n",
    "covariance_counts = cv_results['covariance_type'].value_counts()\n",
    "\n",
    "# Plot parameter frequency\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "n_components_counts.plot(kind='bar', ax=ax1)\n",
    "ax1.set_title('Frequency of Optimal Number of States')\n",
    "ax1.set_xlabel('Number of States')\n",
    "ax1.set_ylabel('Frequency')\n",
    "\n",
    "covariance_counts.plot(kind='bar', ax=ax2)\n",
    "ax2.set_title('Frequency of Optimal Covariance Type')\n",
    "ax2.set_xlabel('Covariance Type')\n",
    "ax2.set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Market Regime Identification\n",
    "\n",
    "One of the key aspects of the HMM model is its ability to identify different market regimes (bull, bear, neutral). Let's analyze how these regimes are characterized and validate their economic interpretation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the HMM model on the full dataset\n",
    "hmm_model = ImprovedHMMModel()\n",
    "hmm_model.fit(spy_data)\n",
    "\n",
    "# Generate predictions\n",
    "predictions = hmm_model.predict(spy_data)\n",
    "\n",
    "# Plot the identified regimes\n",
    "hmm_model.plot_states(predictions, title='SPY Market Regimes')\n",
    "\n",
    "# Analyze regime characteristics\n",
    "regime_stats = {}\n",
    "for state in range(hmm_model.best_params['n_components']):\n",
    "    regime_data = predictions[predictions['Predicted_State'] == state]\n",
    "    \n",
    "    # Skip if no data for this state\n",
    "    if len(regime_data) == 0:\n",
    "        continue\n",
    "    \n",
    "    # Determine regime type\n",
    "    if state == hmm_model.bull_state:\n",
    "        regime_type = 'Bull'\n",
    "    elif state == hmm_model.bear_state:\n",
    "        regime_type = 'Bear'\n",
    "    elif state == hmm_model.neutral_state:\n",
    "        regime_type = 'Neutral'\n",
    "    else:\n",
    "        regime_type = f'State {state}'\n",
    "    \n",
    "    # Calculate statistics\n",
    "    regime_stats[regime_type] = {\n",
    "        'count': len(regime_data),\n",
    "        'pct_of_total': len(regime_data) / len(predictions) * 100,\n",
    "        'avg_daily_return': regime_data['Log_Return'].mean() * 100,\n",
    "        'volatility': regime_data['Log_Return'].std() * 100 * np.sqrt(252),\n",
    "        'sharpe': (regime_data['Log_Return'].mean() / regime_data['Log_Return'].std() * np.sqrt(252)) if regime_data['Log_Return'].std() > 0 else 0,\n",
    "        'win_rate': (regime_data['Log_Return'] > 0).mean() * 100,\n",
    "        'avg_volume': regime_data['Volume'].mean() if 'Volume' in regime_data.columns else None,\n",
    "        'avg_volatility': regime_data['Return_Volatility'].mean() * 100 if 'Return_Volatility' in regime_data.columns else None\n",
    "    }\n",
    "\n",
    "# Display regime statistics\n",
    "regime_stats_df = pd.DataFrame(regime_stats).T\n",
    "print(\"Regime Statistics:\")\n",
    "display(regime_stats_df)\n",
    "\n",
    "# Plot regime return distributions\n",
    "hmm_model.plot_state_distributions(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Regime Transition Analysis\n",
    "\n",
    "Let's analyze the transition probabilities between different market regimes to understand the persistence of each regime and the likelihood of transitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract transition matrix from the model\n",
    "transition_matrix = hmm_model.best_model.transmat_\n",
    "\n",
    "# Create DataFrame for better visualization\n",
    "states = range(hmm_model.best_params['n_components'])\n",
    "transition_df = pd.DataFrame(transition_matrix, index=states, columns=states)\n",
    "\n",
    "# Map state numbers to regime names\n",
    "regime_names = {}\n",
    "for state in states:\n",
    "    if state == hmm_model.bull_state:\n",
    "        regime_names[state] = f\"Bull (State {state})\"\n",
    "    elif state == hmm_model.bear_state:\n",
    "        regime_names[state] = f\"Bear (State {state})\"\n",
    "    elif state == hmm_model.neutral_state:\n",
    "        regime_names[state] = f\"Neutral (State {state})\"\n",
    "    else:\n",
    "        regime_names[state] = f\"State {state}\"\n",
    "\n",
    "# Rename index and columns\n",
    "transition_df.index = [regime_names[state] for state in states]\n",
    "transition_df.columns = [regime_names[state] for state in states]\n",
    "\n",
    "# Display transition matrix\n",
    "print(\"Transition Probability Matrix:\")\n",
    "display(transition_df)\n",
    "\n",
    "# Plot transition matrix as heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(transition_df, annot=True, cmap='YlGnBu', fmt='.3f', linewidths=0.5)\n",
    "plt.title('Regime Transition Probabilities')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate regime persistence\n",
    "persistence = {regime: transition_df.loc[regime, regime] for regime in transition_df.index}\n",
    "persistence_df = pd.DataFrame(list(persistence.items()), columns=['Regime', 'Persistence'])\n",
    "persistence_df['Expected_Duration_Days'] = 1 / (1 - persistence_df['Persistence'])\n",
    "\n",
    "# Plot regime persistence\n",
    "plt.figure(figsize=(12, 6))\n",
    "ax = sns.barplot(x='Regime', y='Expected_Duration_Days', data=persistence_df)\n",
    "plt.title('Expected Duration of Each Market Regime')\n",
    "plt.ylabel('Expected Duration (Trading Days)')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Add value labels\n",
    "for i, v in enumerate(persistence_df['Expected_Duration_Days']):\n",
    "    ax.text(i, v + 0.5, f\"{v:.1f}\", ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Historical Regime Analysis\n",
    "\n",
    "Let's examine how the identified regimes align with known historical market events to validate the model's ability to capture meaningful market states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define major market events\n",
    "market_events = [\n",
    "    {'date': '2010-05-06', 'event': 'Flash Crash', 'type': 'negative'},\n",
    "    {'date': '2011-08-05', 'event': 'US Credit Downgrade', 'type': 'negative'},\n",
    "    {'date': '2015-08-24', 'event': 'China Slowdown Fears', 'type': 'negative'},\n",
    "    {'date': '2016-06-24', 'event': 'Brexit Vote', 'type': 'negative'},\n",
    "    {'date': '2018-02-05', 'event': 'Volatility Spike', 'type': 'negative'},\n",
    "    {'date': '2020-03-23', 'event': 'COVID-19 Market Bottom', 'type': 'positive'},\n",
    "    {'date': '2022-01-03', 'event': 'Fed Tightening Begins', 'type': 'negative'},\n",
    "    {'date': '2023-03-10', 'event': 'Silicon Valley Bank Collapse', 'type': 'negative'}\n",
    "]\n",
    "\n",
    "# Convert dates to datetime\n",
    "for event in market_events:\n",
    "    event['date'] = pd.to_datetime(event['date'])\n",
    "\n",
    "# Function to find the regime around a specific date\n",
    "def get_regime_around_event(predictions, event_date, window=10):\n",
    "    # Find the closest date in the predictions\n",
    "    if event_date not in predictions.index:\n",
    "        closest_date = predictions.index[predictions.index.get_indexer([event_date], method='nearest')[0]]\n",
    "    else:\n",
    "        closest_date = event_date\n",
    "    \n",
    "    # Get index location\n",
    "    event_idx = predictions.index.get_loc(closest_date)\n",
    "    \n",
    "    # Get window around event\n",
    "    start_idx = max(0, event_idx - window)\n",
    "    end_idx = min(len(predictions), event_idx + window + 1)\n",
    "    \n",
    "    # Get data around event\n",
    "    event_window = predictions.iloc[start_idx:end_idx]\n",
    "    \n",
    "    # Count regimes in window\n",
    "    regime_counts = event_window['Market_Regime'].value_counts()\n",
    "    dominant_regime = regime_counts.idxmax() if not regime_counts.empty else 'Unknown'\n",
    "    \n",
    "    # Get regime on event day\n",
    "    event_day_regime = predictions.loc[closest_date, 'Market_Regime'] if closest_date in predictions.index else 'Unknown'\n",
    "    \n",
    "    return {\n",
    "        'event_date': event_date,\n",
    "        'closest_date': closest_date,\n",
    "        'event_day_regime': event_day_regime,\n",
    "        'dominant_regime': dominant_regime,\n",
    "        'regime_counts': regime_counts.to_dict()\n",
    "    }\n",
    "\n",
    "# Analyze regimes around market events\n",
    "event_regimes = []\n",
    "for event in market_events:\n",
    "    regime_info = get_regime_around_event(predictions, event['date'])\n",
    "    event_regimes.append({\n",
    "        'Event': event['event'],\n",
    "        'Date': event['date'],\n",
    "        'Type': event['type'],\n",
    "        'Event Day Regime': regime_info['event_day_regime'],\n",
    "        'Dominant Regime': regime_info['dominant_regime']\n",
    "    })\n",
    "\n",
    "# Display event regimes\n",
    "event_regimes_df = pd.DataFrame(event_regimes)\n",
    "print(\"Market Regimes During Major Events:\")\n",
    "display(event_regimes_df)\n",
    "\n",
    "# Calculate accuracy of regime identification\n",
    "correct_identifications = sum(\n",
    "    (row['Type'] == 'negative' and row['Dominant Regime'] == 'Bear') or\n",
    "    (row['Type'] == 'positive' and row['Dominant Regime'] == 'Bull')\n",
    "    for row in event_regimes\n",
    ")\n",
    "accuracy = correct_identifications / len(event_regimes) * 100\n",
    "\n",
    "print(f\"\\nAccuracy of Regime Identification for Known Events: {accuracy:.2f}%\")\n",
    "\n",
    "# Plot regimes with market events\n",
    "plt.figure(figsize=(16, 8))\n",
    "\n",
    "# Plot price\n",
    "plt.plot(predictions.index, predictions['Close'], color='black', linewidth=1.5)\n",
    "\n",
    "# Color background by regime\n",
    "for state in range(hmm_model.best_params['n_components']):\n",
    "    mask = predictions['Predicted_State'] == state\n",
    "    if state == hmm_model.bull_state:\n",
    "        color = 'green'\n",
    "        alpha = 0.2\n",
    "        label = 'Bull Market'\n",
    "    elif state == hmm_model.bear_state:\n",
    "        color = 'red'\n",
    "        alpha = 0.2\n",
    "        label = 'Bear Market'\n",
    "    elif state == hmm_model.neutral_state:\n",
    "        color = 'blue'\n",
    "        alpha = 0.2\n",
    "        label = 'Neutral Market'\n",
    "    else:\n",
    "        color = 'gray'\n",
    "        alpha = 0.2\n",
    "        label = f'State {state}'\n",
    "    \n",
    "    # Find contiguous regions\n",
    "    regions = []\n",
    "    start_idx = None\n",
    "    \n",
    "    for idx, val in enumerate(mask):\n",
    "        if val and start_idx is None:\n",
    "            start_idx = idx\n",
    "        elif not val and start_idx is not None:\n",
    "            regions.append((start_idx, idx - 1))\n",
    "            start_idx = None\n",
    "    \n",
    "    # Add the last region if it extends to the end\n",
    "    if start_idx is not None:\n",
    "        regions.append((start_idx, len(mask) - 1))\n",
    "    \n",
    "    # Shade each region\n",
    "    for start, end in regions:\n",
    "        plt.axvspan(predictions.index[start], predictions.index[end], \n",
    "                  alpha=alpha, color=color, label=label if start == regions[0][0] else None)\n",
    "\n",
    "# Add market events\n",
    "for event in market_events:\n",
    "    if event['type'] == 'negative':\n",
    "        color = 'red'\n",
    "        marker = 'v'\n",
    "    else:\n",
    "        color = 'green'\n",
    "        marker = '^'\n",
    "    \n",
    "    # Find closest date in data\n",
    "    if event['date'] not in predictions.index:\n",
    "        closest_date = predictions.index[predictions.index.get_indexer([event['date']], method='nearest')[0]]\n",
    "    else:\n",
    "        closest_date = event['date']\n",
    "    \n",
    "    # Get price at event\n",
    "    price = predictions.loc[closest_date, 'Close']\n",
    "    \n",
    "    # Plot event marker\n",
    "    plt.scatter(closest_date, price, color=color, marker=marker, s=100, zorder=5)\n",
    "    \n",
    "    # Add event label\n",
    "    plt.annotate(event['event'], \n",
    "                xy=(closest_date, price),\n",
    "                xytext=(10, 0),\n",
    "                textcoords='offset points',\n",
    "                ha='left',\n",
    "                va='center',\n",
    "                fontsize=9,\n",
    "                rotation=45)\n",
    "\n",
    "plt.title('Market Regimes and Major Events')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Trading Strategy Performance\n",
    "\n",
    "Now let's evaluate the performance of trading strategies based on the HMM model's regime predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate strategy returns\n",
    "predictions['Strategy_Return'] = predictions['Signal'].shift(1) * predictions['Log_Return']\n",
    "predictions['BuyHold_Return'] = predictions['Log_Return']\n",
    "\n",
    "# Calculate cumulative returns\n",
    "predictions['Strategy_Cumulative'] = np.exp(predictions['Strategy_Return'].cumsum()) - 1\n",
    "predictions['BuyHold_Cumulative'] = np.exp(predictions['BuyHold_Return'].cumsum()) - 1\n",
    "\n",
    "# Plot cumulative returns\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(predictions.index, predictions['Strategy_Cumulative'] * 100, label='HMM Strategy', linewidth=2)\n",
    "plt.plot(predictions.index, predictions['BuyHold_Cumulative'] * 100, label='Buy & Hold', linewidth=2, alpha=0.7)\n",
    "plt.title('Cumulative Returns: HMM Strategy vs Buy & Hold')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Cumulative Return (%)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
